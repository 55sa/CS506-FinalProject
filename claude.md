# Boston 311 Service Request Analysis

## Project Overview
Analyzing 14 years (2011-2025) of Boston 311 service requests to identify trends, patterns, and operational insights for city responsiveness improvement.

**Team:** Jiahao He, Thong Dao, Sundeep Routhu, Yijia Chen, Julyssa Michelle Villa Machado

**Current Phase:** Core Analytics Goals (exploratory data analysis and visualization)

## Tech Stack
- **Language:** Python 3.10+
- **Data Processing:** pandas, numpy
- **Visualization:** matplotlib, seaborn, plotly (for interactive maps)
- **Geospatial:** geopandas, folium
- **Logging:** Python standard library `logging`
- **Environment:** Jupyter notebooks for exploration, .py scripts for production analysis

## Code Style Guide
**ðŸ“– See [CODE_STYLE.md](CODE_STYLE.md) for complete coding standards.**

Key principles:
- One function = one task (â‰¤25 lines preferred)
- Analysis functions: compute â†’ return DataFrame (NO plotting)
- Visualization functions: receive data â†’ render + save plot
- Use one-line docstrings for simple functions, full docstrings for complex ones
- Type hints required on all function signatures



## Project Structure
```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Downloaded CSV files (automated via download_data.py)
â”‚   â”‚   â”œâ”€â”€ 311_requests_2011.csv
â”‚   â”‚   â”œâ”€â”€ 311_requests_2012.csv
â”‚   â”‚   â””â”€â”€ ... (through 2025)
â”‚   â””â”€â”€ processed/        # Cleaned and merged datasets (generated)
â”‚       â””â”€â”€ 311_cleaned.csv  # Output from preprocessor.py
â”œâ”€â”€ notebooks/            # Exploratory analysis (Jupyter)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core_analysis.py  # ðŸŽ¯ MAIN SCRIPT: Generates all 15 visualizations
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py     # Load and merge yearly CSV files
â”‚   â”‚   â””â”€â”€ preprocessor.py # Clean and derive features
â”‚   â”œâ”€â”€ analysis/         # ðŸ“Š Analysis functions (NO visualization)
â”‚   â”‚   â”œâ”€â”€ temporal.py   # Year-over-year trends, daily averages
â”‚   â”‚   â”œâ”€â”€ categorical.py # Request types by neighborhood, dept
â”‚   â”‚   â””â”€â”€ resolution.py  # Resolution time calculations
â”‚   â””â”€â”€ visualization/    # ðŸ“ˆ Visualization functions (plotting only)
â”‚       â”œâ”€â”€ maps.py       # Choropleth and heatmaps
â”‚       â”œâ”€â”€ temporal.py   # Time series plots
â”‚       â””â”€â”€ comparative.py # Bar charts, scatter plots
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/          # Generated PNG charts (15 files, 300 DPI)
â”‚   â””â”€â”€ reports/          # Analysis summaries
â”œâ”€â”€ download_data.py      # Automated data download script
â”œâ”€â”€ claude.md             # This file - project instructions
â”œâ”€â”€ GETTING_STARTED.md    # Quick start guide
â”œâ”€â”€ README.md             # Full documentation
â””â”€â”€ requirements.txt      # Python dependencies
```

### Key Directories & Files
- **`src/core_analysis.py`** - ðŸŽ¯ **MAIN SCRIPT** - Run this to generate all 15 visualizations
- **`data/raw/`** - Downloaded CSV files (use `python download_data.py` to get them)
- **`data/processed/`** - Generated by preprocessor (merged/cleaned data)
- **`src/analysis/`** - âš ï¸ **IMPORTANT:** Analysis functions return data, NO plotting
- **`src/visualization/`** - Visualization functions create plots from data
- **`outputs/figures/`** - Generated visualizations (numbered 1-15)

I'll update the claude.md with the correct data schema from the PDF. Here's the corrected section:

Add this to replace the "Data Schema (Key Columns)" section in your claude.md:
markdown## Data Schema (Key Columns)

Based on Boston 311 dataset documentation:

### Core Identifiers & Timestamps
- `case_enquiry_id` - unique request ID
- `open_dt` - request submission timestamp
- `closed_dt` - case closure timestamp (nullable)
- `case_status` - open/closed/null
- `case_title` - free-text title entered by call taker (e.g., "Schedule a Bulk Item Pickup", "Graffiti", "Sidewalk repair")

### Case Classification Hierarchy (SUBJECT > REASON > TYPE)
- `subject` - Department assigned to (e.g., "Public Works Department", "Transportation - Traffic Division", "Inspectional Services")
- `reason` - Umbrella category for similar cases (e.g., "Street Lights", "Sanitation", "Code Enforcement", "Graffiti")
- `type` - Specific case type entered by call taker (e.g., "BTDT: Complaint", "Graffiti: Ward 1 0106 Profane Text")

### Queue & Department
- `queue` - Processing queue within department (e.g., "PWDx_Street Light Outages", "ISD_Code Enforcement (INTERNAL)")
- `department` - Derived from first 4 chars of queue (PWDx=Public Works, BTDT=Transportation, ISD=Inspectional Services, etc.)

### Closure Information
- `closure_reason` - Status codes like:
  - Resolved, Noted, Closed
  - NOACC (No Access), TFA (Tenant Failed Apt)
  - PERMIS (Permit issued), NOBASE (No basis)
  - CORR (Corrected), ROA (Refer to other agency)
  - TICKET, VIOCOR (Violation Corrected), VIOISS (Violation filed)
  - Duplicate, Case Invalid, Scheduled

### Geographic Fields
- `neighborhood` - Neighborhood name (e.g., "Allston", "Back Bay", "Dorchester")
  - **Note:** Inconsistent naming across years, some redundancies
- `location_zipcode` - Boston zip codes (02108-02467)
- `location` - lat/long coordinates (nullable)

### District Boundaries
- `fire_district` - BFD districts (1, 3, 4, 6, 7, 8, 9, 11, 12)
- `pwd_district` - Public Works districts (1-10)
- `city_council_district` - City Council districts (1-9)
- `police_district` - BPD districts (A-1, A-7, B-2, C-6, D-4, E-5, etc.)
- `neighborhood_services_district` - Office of Neighborhood Services districts (0-15)
- `ward` - Election ward
- `precinct` - Election precinct

### Additional Fields
- `source` - Submission channel (web, phone, mobile, etc.)
- `land_usage` - Zoning (R1=Residential 1 Family, C=Commercial, I=Industrial, etc.)

### Data Quality Notes
- **CLOSED_DT:** Null for open cases or missing data (~30% expected)
- **CASE_STATUS:** Can be null (separate category in analysis)
- **Neighborhood:** Inconsistent naming (e.g., "Allston" vs "Allston / Brighton")
- **Coordinates:** Some records missing location data
- **SOURCE:** Categories may change over time as new channels added
- **Hierarchy:** SUBJECT > REASON > TYPE provides 3-level classification

## Code Style & Conventions

### Python Style
- Follow PEP 8 (standard Python style guide)
- Use descriptive variable names: `total_requests_per_year`, not `tr`
- Functions: lowercase_with_underscores
- Classes: PascalCase (if needed)
- Constants: UPPER_SNAKE_CASE

### Data Processing Patterns
```python
# Prefer method chaining for pandas operations
df = (df
      .dropna(subset=['open_dt'])
      .assign(year=lambda x: pd.to_datetime(x['open_dt']).dt.year)
      .groupby('year')
      .size()
)

# Use meaningful intermediate variables for complex operations
resolution_time = (df['closed_dt'] - df['open_dt']).dt.total_seconds() / 3600  # hours
df['resolution_hours'] = resolution_time
```

### Logging
```python
import logging

# Setup at module level
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Usage
logger.info(f"Loaded {len(df)} records from {filepath}")
logger.warning(f"Found {missing_count} records with missing CLOSED_DT")
```

### Separation of Concerns
- **data/loader.py:** Only load and merge raw CSV files, return DataFrame
- **data/preprocessor.py:** Clean data, handle nulls, derive temporal features
- **analysis/*.py:** âš ï¸ **CRITICAL:** Compute metrics, aggregations - return data structures (Series, DataFrame, dicts) - **NEVER create plots**
- **visualization/*.py:** Create plots from processed data - **NEVER compute analysis logic**
- **core_analysis.py:** Orchestrates analysis + visualization - uses functions from both modules

### Code Reuse Guidelines
- âœ… **DO:** Use analysis functions from `src/analysis/` in core_analysis.py
- âœ… **DO:** Import `calculate_requests_per_year()`, `calculate_top_neighborhoods()`, etc.
- âŒ **DON'T:** Write inline analysis logic when a function already exists in `src/analysis/`
- âŒ **DON'T:** Put analysis calculations in visualization functions

## Current Focus: Core Analytics Goals

### âœ… COMPLETED - Core Analytics Implementation
All core analytics goals have been implemented in `src/core_analysis.py`:

1. âœ… Total volume of requests per year - `calculate_requests_per_year()`
2. âœ… Most common request types overall - `calculate_request_types_overall()`
3. âœ… Request types by neighborhood - uses `calculate_top_neighborhoods()`
4. âœ… Trends by SUBJECT - `calculate_trends_by_subject()`
5. âœ… Trends by REASON - inline groupby (consider adding to temporal.py)
6. âœ… Trends by QUEUE - inline groupby (consider adding to categorical.py)
7. âœ… Case volume by SOURCE - `calculate_source_distribution()`
8. âœ… Average daily contacts by year - `calculate_average_daily_contacts()`
9. âœ… Top 5 request types volume - inline analysis
10. âœ… Average resolution time by QUEUE - `calculate_average_resolution_by_queue()`
11. âœ… Resolution time by QUEUE and neighborhood - uses resolution module
12. âœ… Case status breakdown - `calculate_case_status_breakdown()`

**To run:** `python src/core_analysis.py`
**Output:** 15 PNG visualizations in `outputs/figures/`

### What NOT to Do (Yet)
- âŒ No machine learning models (that's Phase 2)
- âŒ No predictions or forecasting
- âŒ No clustering or segmentation
- âŒ No database queries (work with CSV files directly)
- âŒ No unit tests for now (exploratory phase)

## Data Handling Preferences

### Missing Data
- **CLOSED_DT:** Common for open cases - handle gracefully, don't drop
- **CASE_STATUS null:** Track as separate category in analysis
- **Location:** Some records may lack coordinates - analyze with and without

### Temporal Features to Derive
```python
df['year'] = pd.to_datetime(df['open_dt']).dt.year
df['month'] = pd.to_datetime(df['open_dt']).dt.month
df['day_of_week'] = pd.to_datetime(df['open_dt']).dt.day_name()
df['hour'] = pd.to_datetime(df['open_dt']).dt.hour

# Resolution time (only for closed cases)
df['resolution_hours'] = (
    (pd.to_datetime(df['closed_dt']) - pd.to_datetime(df['open_dt']))
    .dt.total_seconds() / 3600
)
```

### Aggregations
- Use `.groupby()` extensively for neighborhood, year, type breakdowns
- Always include `.size()` or `.count()` for volume metrics
- Use `.mean()`, `.median()` for resolution times
- Calculate percentages with `.value_counts(normalize=True)`

## Visualization Preferences

### Chart Types for Each Goal
- **Volume trends:** Line charts (year over year)
- **Request types:** Horizontal bar charts (easier to read labels)
- **Neighborhood comparisons:** Grouped/stacked bar charts
- **Resolution times:** Box plots or violin plots (show distribution)
- **Status breakdown:** Pie charts or stacked bar charts
- **Geographic:** Choropleth maps (per-capita rates by neighborhood)

### Styling
- Use seaborn default theme or 'whitegrid'
- Color palette: 'Set2' or 'viridis' for continuous data
- Always include:
  - Clear titles
  - Axis labels with units
  - Legends when multiple series
  - Grid lines for readability

### Output
- Save figures to `outputs/figures/` as PNG (300 DPI for reports)
- Use descriptive filenames: `requests_per_year_2011_2025.png`

## Important Context

### Data Source
- Download CSVs from: https://data.boston.gov/dataset/311-service-requests
- Separate files per year (2011-2025)
- ~100k-500k records per year
- Total dataset: ~5M records

### Known Issues
- **CLOSED_DT:** ~30% of records may be null (open cases or missing data)
- **Neighborhood:** Some inconsistent naming across years
- **Coordinates:** Not all records have location data
- **SOURCE:** Categories may change over time (new submission channels added)

### Performance Considerations
- Dataset is ~5M rows - pandas can handle in-memory
- Use `.query()` for filtering instead of boolean indexing when possible
- Cache processed dataframes to avoid re-processing

## How to Help Me

### Code Generation
- Write modular functions (one function = one metric)
- Include docstrings with parameter types and return values
- Add inline comments for complex pandas operations

### Analysis Approach
- Start with simple aggregations, then drill down
- Always validate: print row counts, check for nulls, inspect distributions
- Show me intermediate results to verify correctness

### Explanations
- Explain pandas operations step-by-step (especially chaining)
- Point out potential data quality issues
- Suggest alternative approaches when relevant

## Example Function Style I Want
```python
def calculate_requests_per_year(df: pd.DataFrame) -> pd.Series:
    """
    Calculate total number of 311 requests per year.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame with 'open_dt' column
        
    Returns:
    --------
    pd.Series
        Index: year, Values: request count
    """
    logger.info("Calculating requests per year")
    
    # Extract year from open_dt
    df['year'] = pd.to_datetime(df['open_dt']).dt.year
    
    # Group and count
    yearly_counts = df.groupby('year').size()
    
    logger.info(f"Found data for years: {yearly_counts.index.tolist()}")
    
    return yearly_counts
```

## Type Checking & Code Quality

### Pylance Configuration
We use **basic** mode (not strict) for Pylance type checking:

```json
// .vscode/settings.json
{
  "python.analysis.typeCheckingMode": "basic",
  "python.analysis.diagnosticSeverityOverrides": {
    "reportCallIssue": "none",      // Suppresses matplotlib/pandas overload warnings
    "reportArgumentType": "none"     // Suppresses ArrayLike type warnings
  }
}
```

**Why basic mode?**
- Standard for data science projects
- Avoids false positives with pandas/matplotlib type stubs
- Still catches real errors (undefined variables, wrong imports)

### Type Hints
Use type hints for function signatures:

```python
from __future__ import annotations
import pandas as pd

def calculate_requests_per_year(df: pd.DataFrame) -> pd.Series:
    """Calculate total requests per year."""
    return df.groupby('year').size()
```

**What to type hint:**
- âœ… Function parameters and return types
- âœ… Complex variables: `queue_stats: pd.DataFrame = ...`
- âŒ Don't over-annotate: intermediate variables in simple functions

### Matplotlib Type Issues
When you see `"ArrayLike" cannot be assigned` errors:

**Quick fix:** Already suppressed in `.vscode/settings.json`

**Proper import:**
```python
from matplotlib.ticker import FuncFormatter  # âœ… Correct
# NOT plt.FuncFormatter  # âŒ Wrong - triggers Pylance error
```

See `MATPLOTLIB_TYPE_ISSUES.md` for details.

### Linting Tools
Configured in `pyproject.toml`:
- **Black** - Auto-formatter (100 char line length)
- **isort** - Import organizer
- **flake8** - Style checker
- **mypy** - Type checker (relaxed mode)

**To format code:**
```bash
black src/
isort src/
```

## Don't Do This
- âŒ Don't put analysis logic in visualization functions
- âŒ Don't hardcode file paths (use config or parameters)
- âŒ Don't use magic numbers (define constants: `HOURS_PER_DAY = 24`)
- âŒ Don't suppress warnings without understanding them
- âŒ Don't print() for debugging (use logger.debug())
- âŒ Don't use Pylance strict mode (too verbose for pandas workflows)
- âŒ Don't import FuncFormatter from plt (use matplotlib.ticker)

## Questions to Ask Me
When implementing features, consider asking:
- "Should we filter out null CASE_STATUS records or analyze them separately?"
- "For resolution time, should we exclude outliers (>X days)?"
- "Which neighborhoods should we prioritize for the initial analysis?"
- "Do you want to see absolute counts or per-capita rates?"

## Quick Reference

### Common Commands
```bash
# Download data (one-time setup)
python download_data.py

# Generate all 15 visualizations
python src/core_analysis.py

# Preprocess data only
python src/data/preprocessor.py

# Format code
black src/
isort src/

# Type check
mypy src/
```

### File Naming Convention
- Data files: `311_requests_YYYY.csv` (e.g., `311_requests_2024.csv`)
- Visualization outputs: `N_description.png` (e.g., `1_requests_per_year.png`)
- Scripts: `lowercase_with_underscores.py`

### Key Insights from Current Analysis
Based on 3.2M records (2011-2025):
- **Peak year:** 2024 (282,836 requests)
- **Top request type:** Parking Enforcement (14.7%)
- **Resolution rate:** 90.95% closed
- **Average resolution:** 12.1 days (median: 0.6 days)
- **Busiest neighborhood:** Dorchester (14.3%)
- **Main channel:** Citizens Connect App (38.3%)
